{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 37237 samples.\n",
      "Example text: word katandandre food crapilicious\n",
      "Example label: 3\n",
      "Shape of padded input data: (37237, 43)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        X (pd.DataFrame): Features (tweet texts).\n",
    "        y (pd.Series): Target labels (cyberbullying types).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    X = df['text_cleaned'].values\n",
    "    y = df['cyberbullying_type'].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 2. Preprocess data (convert to numerical format)\n",
    "def preprocess_labels(y):\n",
    "    \"\"\"Encodes target labels as integers.\n",
    "    \n",
    "    Args:\n",
    "        y (np.array or pd.Series): Target labels.\n",
    "    \n",
    "    Returns:\n",
    "        y_encoded (np.array): Encoded labels.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "file_path = '../data/cleaned_data.csv' \n",
    "\n",
    "# Load and preprocess the data\n",
    "X, y = load_data(file_path)\n",
    "y = preprocess_labels(y)\n",
    "\n",
    "# Print a summary of the loaded data\n",
    "print(f\"Loaded {len(X)} samples.\")\n",
    "print(f\"Example text: {X[0]}\")\n",
    "print(f\"Example label: {y[0]}\")\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()  \n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "max_sequence_length = max([len(seq) for seq in X_tokenized])\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=max_sequence_length)\n",
    "\n",
    "# Print shape of the processed input data\n",
    "print(f\"Shape of padded input data: {X_padded.shape}\")\n",
    "\n",
    "# Update X for model training\n",
    "X = X_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(vocab_size, embedding_dim=128, rnn_units=128, dropout_rate=0.4, output_classes=6, activation='softmax', stacked_rnn_layers=False):\n",
    "    \"\"\"Builds a SimpleRNN model with specified parameters, including an optional additional Dense layer.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embedding_dim (int): Dimensionality of embedding vectors.\n",
    "        rnn_units (int): Number of units in the RNN layer.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "        output_classes (int): Number of output classes.\n",
    "        activation (str): Activation function for the output layer.\n",
    "        additional_layer (bool): Whether to add an additional Dense layer.\n",
    "    \n",
    "    Returns:\n",
    "        model (keras.Model): Compiled RNN model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "\n",
    "    if stacked_rnn_layers:\n",
    "        model.add(SimpleRNN(rnn_units, return_sequences=True))\n",
    "        model.add(SimpleRNN(rnn_units, return_sequences=False))\n",
    "    else:\n",
    "        model.add(SimpleRNN(rnn_units, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_classes, activation=activation))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0003)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'models' directory if it doesn't exist\n",
    "models_dir = 'models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: Baseline RNN\n",
      "Epoch 1/10\n",
      "745/745 - 39s - 52ms/step - accuracy: 0.7677 - loss: 0.6293 - val_accuracy: 0.9164 - val_loss: 0.2325\n",
      "Epoch 2/10\n",
      "745/745 - 30s - 41ms/step - accuracy: 0.9355 - loss: 0.1934 - val_accuracy: 0.9317 - val_loss: 0.2028\n",
      "Epoch 3/10\n",
      "745/745 - 29s - 39ms/step - accuracy: 0.9629 - loss: 0.1146 - val_accuracy: 0.9278 - val_loss: 0.2230\n",
      "Epoch 4/10\n",
      "745/745 - 28s - 38ms/step - accuracy: 0.9772 - loss: 0.0765 - val_accuracy: 0.9228 - val_loss: 0.2368\n",
      "Epoch 5/10\n",
      "745/745 - 31s - 41ms/step - accuracy: 0.9818 - loss: 0.0609 - val_accuracy: 0.9258 - val_loss: 0.2482\n",
      "Test Score: loss of 0.2237604707479477; compile_metrics of 92.18581914901733%\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1582\n",
      "           1       0.98      0.98      0.98      1558\n",
      "           2       0.90      0.87      0.89      1489\n",
      "           3       0.77      0.82      0.80      1237\n",
      "           4       0.95      0.94      0.95      1582\n",
      "\n",
      "    accuracy                           0.92      7448\n",
      "   macro avg       0.92      0.92      0.92      7448\n",
      "weighted avg       0.92      0.92      0.92      7448\n",
      "\n",
      "Model saved to models/Baseline_RNN.keras\n",
      "\n",
      "Running experiment: RNN with SMOTE\n",
      "Epoch 1/10\n",
      "791/791 - 35s - 44ms/step - accuracy: 0.7358 - loss: 0.7053 - val_accuracy: 0.9162 - val_loss: 0.2400\n",
      "Epoch 2/10\n",
      "791/791 - 30s - 38ms/step - accuracy: 0.9191 - loss: 0.2467 - val_accuracy: 0.9272 - val_loss: 0.2190\n",
      "Epoch 3/10\n",
      "791/791 - 30s - 38ms/step - accuracy: 0.9510 - loss: 0.1563 - val_accuracy: 0.9243 - val_loss: 0.2319\n",
      "Epoch 4/10\n",
      "791/791 - 29s - 37ms/step - accuracy: 0.9680 - loss: 0.1045 - val_accuracy: 0.9136 - val_loss: 0.2771\n",
      "Epoch 5/10\n",
      "791/791 - 30s - 37ms/step - accuracy: 0.9819 - loss: 0.0626 - val_accuracy: 0.9114 - val_loss: 0.3130\n",
      "Test Score: loss of 0.24306616187095642; compile_metrics of 91.89044237136841%\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      1582\n",
      "           1       0.99      0.96      0.98      1558\n",
      "           2       0.95      0.81      0.88      1489\n",
      "           3       0.73      0.91      0.81      1237\n",
      "           4       0.94      0.94      0.94      1582\n",
      "\n",
      "    accuracy                           0.92      7448\n",
      "   macro avg       0.92      0.92      0.92      7448\n",
      "weighted avg       0.93      0.92      0.92      7448\n",
      "\n",
      "Model saved to models/RNN_with_SMOTE.keras\n",
      "\n",
      "Running experiment: RNN with stacked RNN Layer\n",
      "Epoch 1/10\n",
      "745/745 - 36s - 48ms/step - accuracy: 0.7793 - loss: 0.5786 - val_accuracy: 0.9204 - val_loss: 0.2300\n",
      "Epoch 2/10\n",
      "745/745 - 30s - 41ms/step - accuracy: 0.9445 - loss: 0.1706 - val_accuracy: 0.9209 - val_loss: 0.2311\n",
      "Epoch 3/10\n",
      "745/745 - 31s - 42ms/step - accuracy: 0.9740 - loss: 0.0867 - val_accuracy: 0.9149 - val_loss: 0.2769\n",
      "Epoch 4/10\n",
      "745/745 - 31s - 42ms/step - accuracy: 0.9765 - loss: 0.0749 - val_accuracy: 0.9231 - val_loss: 0.2765\n",
      "Test Score: loss of 0.2515592575073242; compile_metrics of 91.32652878761292%\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      1582\n",
      "           1       0.97      0.98      0.97      1558\n",
      "           2       0.94      0.82      0.87      1489\n",
      "           3       0.74      0.85      0.79      1237\n",
      "           4       0.94      0.94      0.94      1582\n",
      "\n",
      "    accuracy                           0.91      7448\n",
      "   macro avg       0.91      0.91      0.91      7448\n",
      "weighted avg       0.92      0.91      0.91      7448\n",
      "\n",
      "Model saved to models/RNN_with_stacked_RNN_Layer.keras\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(X, y, model_builder, save_path=None, test_size=0.2, val_size=0.2, oversample=False):\n",
    "    \"\"\"Trains and evaluates the model using a train-validation-test split and saves the model.\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): Padded input data.\n",
    "        y (np.array): Encoded target labels.\n",
    "        model_builder (function): Function to build the model.\n",
    "        save_path (str): Directory path to save the model. If None, the model is not saved.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        val_size (float): Proportion of the training data to include in the validation split.\n",
    "        oversample (bool): Whether to apply SMOTE for oversampling.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "    \n",
    "    # Further split the training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42, stratify=y_train)\n",
    "    \n",
    "    # Optionally oversample the training data using SMOTE\n",
    "    if oversample:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = model_builder()\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test Score: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    \n",
    "    # Store true labels and predicted labels for classification report\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f'\\nClassification Report:\\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model if save_path is provided\n",
    "    if save_path:\n",
    "        model.save(save_path)\n",
    "        print(f'Model saved to {save_path}')\n",
    "\n",
    "\n",
    "# Configurations for experiments\n",
    "experiments = [\n",
    "    {\"name\": \"Baseline RNN\", \"builder\": lambda: build_rnn_model(vocab_size=len(tokenizer.word_index) + 1)},\n",
    "    {\"name\": \"RNN with SMOTE\", \"builder\": lambda: build_rnn_model(vocab_size=len(tokenizer.word_index) + 1), \"oversample\": True},\n",
    "    {\"name\": \"RNN with stacked RNN Layer\", \"builder\": lambda: build_rnn_model(vocab_size=len(tokenizer.word_index) + 1, stacked_rnn_layers=True)}\n",
    "]\n",
    "\n",
    "# Execute experiments with model saving\n",
    "for experiment in experiments:\n",
    "    print(f\"\\nRunning experiment: {experiment['name']}\")\n",
    "    save_path = f\"models/{experiment['name'].replace(' ', '_')}.keras\"\n",
    "    train_and_evaluate_model(X, y, experiment[\"builder\"], save_path=save_path, oversample=experiment.get(\"oversample\", False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "* SMOTE Impact: SMOTE improved the balance between classes but didn't significantly improve the overall accuracy. It seems to have slightly worsened performance on some classes.\n",
    "\n",
    "* Stacked RNN Layers: Adding stacked RNN layers improved the model’s accuracy slightly compared to the baseline but didn't drastically change the results. It did, however, help with class 4's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
